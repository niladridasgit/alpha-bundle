{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6909be6b-c677-4f65-b25d-25320d03d634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Databricks Asset Bundles\n",
    "Databricks Asset Bundles describe <b>Databricks resources such as jobs and pipelines as source files</b> and allow us to include metadata alongside these source files to provision these resources to provide an end-to-end definition of a project by packaging whole infrastructure as a single deployable project.\n",
    "\n",
    "<br>\n",
    "<img src=\"/Workspace/Users/niladridasgit@gmail.com/databricks_nd_git/images/databricks-asset-bundle-architecture.png\" alt=\"Databricks Asset Bundle [diagram]\" width=\"100%\">\n",
    "\n",
    "A Databricks Asset Bundle can be compared to a courier package that contains multiple component(s) (asset(s)) to transport those item(s) from one location (development environment) to another location (production environment). Basically, a Databricks Asset Bundle is used to lift and shift Databricks resource(s) or asset(s) from a one environment to another environment.\n",
    "\n",
    "![](/Workspace/Users/niladridasgit@gmail.com/databricks_nd_git/images/databricks-asset-bundle-comparison-with-real-world-example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\niladri.b.das\\documents\\alpha\\alpha-bundle\\.venv\\lib\\site-packages (25.3)\n",
      "Collecting pip\n",
      "  Downloading pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-26.0.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 5.6 MB/s  0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.3\n",
      "    Uninstalling pip-25.3:\n",
      "      Successfully uninstalled pip-25.3\n",
      "Successfully installed pip-26.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-sdk in .\\.venv\\Lib\\site-packages (0.86.0)\n",
      "Requirement already satisfied: requests<3,>=2.28.1 in .\\.venv\\Lib\\site-packages (from databricks-sdk) (2.32.5)\n",
      "Requirement already satisfied: google-auth~=2.0 in .\\.venv\\Lib\\site-packages (from databricks-sdk) (2.48.0)\n",
      "Requirement already satisfied: protobuf!=5.26.*,!=5.27.*,!=5.28.*,!=5.29.0,!=5.29.1,!=5.29.2,!=5.29.3,!=5.29.4,!=6.30.0,!=6.30.1,!=6.31.0,<7.0,>=4.25.8 in .\\.venv\\Lib\\site-packages (from databricks-sdk) (6.33.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in .\\.venv\\Lib\\site-packages (from google-auth~=2.0->databricks-sdk) (0.4.2)\n",
      "Requirement already satisfied: cryptography>=38.0.3 in .\\.venv\\Lib\\site-packages (from google-auth~=2.0->databricks-sdk) (46.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in .\\.venv\\Lib\\site-packages (from google-auth~=2.0->databricks-sdk) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in .\\.venv\\Lib\\site-packages (from requests<3,>=2.28.1->databricks-sdk) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\Lib\\site-packages (from requests<3,>=2.28.1->databricks-sdk) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\Lib\\site-packages (from requests<3,>=2.28.1->databricks-sdk) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\Lib\\site-packages (from requests<3,>=2.28.1->databricks-sdk) (2026.1.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in .\\.venv\\Lib\\site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk) (0.6.2)\n",
      "Requirement already satisfied: cffi>=2.0.0 in .\\.venv\\Lib\\site-packages (from cryptography>=38.0.3->google-auth~=2.0->databricks-sdk) (2.0.0)\n",
      "Requirement already satisfied: pycparser in .\\.venv\\Lib\\site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth~=2.0->databricks-sdk) (3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install databricks-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully authenticated to Databricks workspace!\n",
      "Workspace ID: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# os.environ['DATABRICKS_HOST'] = 'https://dbc-1d41a383-c4d0.cloud.databricks.com/'\n",
    "# os.environ['DATABRICKS_TOKEN'] = ''\n",
    "\n",
    "# Initialize and authenticate to Databricks workspace\n",
    "try:\n",
    "    client = WorkspaceClient()\n",
    "    \n",
    "    # Test the connection by getting workspace info\n",
    "    workspace_id = client.workspace.get_status(path=\"/\").object_id\n",
    "    print(f\"✓ Successfully authenticated to Databricks workspace!\")\n",
    "    print(f\"Workspace ID: {workspace_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Authentication failed: {e}\")\n",
    "    print(\"\\nPlease ensure:\")\n",
    "    print(\"1. DATABRICKS_HOST and DATABRICKS_TOKEN environment variables are set, OR\")\n",
    "    print(\"2. ~/.databrickscfg file exists with your workspace credentials\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class WorkspaceClient in module databricks.sdk:\n",
      "\n",
      "class WorkspaceClient(builtins.object)\n",
      " |  WorkspaceClient(\n",
      " |      *,\n",
      " |      host: Optional[str] = None,\n",
      " |      account_id: Optional[str] = None,\n",
      " |      username: Optional[str] = None,\n",
      " |      password: Optional[str] = None,\n",
      " |      client_id: Optional[str] = None,\n",
      " |      client_secret: Optional[str] = None,\n",
      " |      token: Optional[str] = None,\n",
      " |      profile: Optional[str] = None,\n",
      " |      config_file: Optional[str] = None,\n",
      " |      azure_workspace_resource_id: Optional[str] = None,\n",
      " |      azure_client_secret: Optional[str] = None,\n",
      " |      azure_client_id: Optional[str] = None,\n",
      " |      azure_tenant_id: Optional[str] = None,\n",
      " |      azure_environment: Optional[str] = None,\n",
      " |      auth_type: Optional[str] = None,\n",
      " |      cluster_id: Optional[str] = None,\n",
      " |      google_credentials: Optional[str] = None,\n",
      " |      google_service_account: Optional[str] = None,\n",
      " |      debug_truncate_bytes: Optional[int] = None,\n",
      " |      debug_headers: Optional[bool] = None,\n",
      " |      product='unknown',\n",
      " |      product_version='0.0.0',\n",
      " |      credentials_strategy: Optional[CredentialsStrategy] = None,\n",
      " |      credentials_provider: Optional[CredentialsStrategy] = None,\n",
      " |      token_audience: Optional[str] = None,\n",
      " |      config: Optional[client.Config] = None,\n",
      " |      scopes: Optional[List[str]] = None,\n",
      " |      authorization_details: Optional[List[AuthorizationDetail]] = None,\n",
      " |      custom_headers: Optional[dict] = None\n",
      " |  )\n",
      " |\n",
      " |  The WorkspaceClient is a client for the workspace-level Databricks REST API.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      *,\n",
      " |      host: Optional[str] = None,\n",
      " |      account_id: Optional[str] = None,\n",
      " |      username: Optional[str] = None,\n",
      " |      password: Optional[str] = None,\n",
      " |      client_id: Optional[str] = None,\n",
      " |      client_secret: Optional[str] = None,\n",
      " |      token: Optional[str] = None,\n",
      " |      profile: Optional[str] = None,\n",
      " |      config_file: Optional[str] = None,\n",
      " |      azure_workspace_resource_id: Optional[str] = None,\n",
      " |      azure_client_secret: Optional[str] = None,\n",
      " |      azure_client_id: Optional[str] = None,\n",
      " |      azure_tenant_id: Optional[str] = None,\n",
      " |      azure_environment: Optional[str] = None,\n",
      " |      auth_type: Optional[str] = None,\n",
      " |      cluster_id: Optional[str] = None,\n",
      " |      google_credentials: Optional[str] = None,\n",
      " |      google_service_account: Optional[str] = None,\n",
      " |      debug_truncate_bytes: Optional[int] = None,\n",
      " |      debug_headers: Optional[bool] = None,\n",
      " |      product='unknown',\n",
      " |      product_version='0.0.0',\n",
      " |      credentials_strategy: Optional[CredentialsStrategy] = None,\n",
      " |      credentials_provider: Optional[CredentialsStrategy] = None,\n",
      " |      token_audience: Optional[str] = None,\n",
      " |      config: Optional[client.Config] = None,\n",
      " |      scopes: Optional[List[str]] = None,\n",
      " |      authorization_details: Optional[List[AuthorizationDetail]] = None,\n",
      " |      custom_headers: Optional[dict] = None\n",
      " |  )\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  get_workspace_id(self) -> int\n",
      " |      Get the workspace ID of the workspace that this client is connected to.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  access_control\n",
      " |      Rule based Access Control for Databricks Resources.\n",
      " |\n",
      " |  account_access_control_proxy\n",
      " |      These APIs manage access rules on resources in an account.\n",
      " |\n",
      " |  agent_bricks\n",
      " |      The Custom LLMs service manages state and powers the UI for the Custom LLM product.\n",
      " |\n",
      " |  alerts\n",
      " |      The alerts API can be used to perform CRUD operations on alerts.\n",
      " |\n",
      " |  alerts_legacy\n",
      " |      The alerts API can be used to perform CRUD operations on alerts.\n",
      " |\n",
      " |  alerts_v2\n",
      " |      New version of SQL Alerts.\n",
      " |\n",
      " |  api_client\n",
      " |\n",
      " |  apps\n",
      " |      Apps run directly on a customer's Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on.\n",
      " |\n",
      " |  apps_settings\n",
      " |      Apps Settings manage the settings for the Apps service on a customer's Databricks instance.\n",
      " |\n",
      " |  artifact_allowlists\n",
      " |      In Databricks Runtime 13.3 and above, you can add libraries and init scripts to the `allowlist` in UC so that users can leverage these artifacts on compute configured with shared access mode.\n",
      " |\n",
      " |  catalogs\n",
      " |      A catalog is the first layer of Unity Catalog’s three-level namespace.\n",
      " |\n",
      " |  clean_room_asset_revisions\n",
      " |      Clean Room Asset Revisions denote new versions of uploaded assets (e.g.\n",
      " |\n",
      " |  clean_room_assets\n",
      " |      Clean room assets are data and code objects — Tables, volumes, and notebooks that are shared with the clean room.\n",
      " |\n",
      " |  clean_room_auto_approval_rules\n",
      " |      Clean room auto-approval rules automatically create an approval on your behalf when an asset (e.g.\n",
      " |\n",
      " |  clean_room_task_runs\n",
      " |      Clean room task runs are the executions of notebooks in a clean room.\n",
      " |\n",
      " |  clean_rooms\n",
      " |      A clean room uses Delta Sharing and serverless compute to provide a secure and privacy-protecting environment where multiple parties can work together on sensitive enterprise data without direct access to each other's data.\n",
      " |\n",
      " |  cluster_policies\n",
      " |      You can use cluster policies to control users' ability to configure clusters based on a set of rules.\n",
      " |\n",
      " |  clusters\n",
      " |      The Clusters API allows you to create, start, edit, list, terminate, and delete clusters.\n",
      " |\n",
      " |  command_execution\n",
      " |      This API allows execution of Python, Scala, SQL, or R commands on running Databricks Clusters.\n",
      " |\n",
      " |  config\n",
      " |\n",
      " |  connections\n",
      " |      Connections allow for creating a connection to an external data source.\n",
      " |\n",
      " |  consumer_fulfillments\n",
      " |      Fulfillments are entities that allow consumers to preview installations.\n",
      " |\n",
      " |  consumer_installations\n",
      " |      Installations are entities that allow consumers to interact with Databricks Marketplace listings.\n",
      " |\n",
      " |  consumer_listings\n",
      " |      Listings are the core entities in the Marketplace.\n",
      " |\n",
      " |  consumer_personalization_requests\n",
      " |      Personalization Requests allow customers to interact with the individualized Marketplace listing flow.\n",
      " |\n",
      " |  consumer_providers\n",
      " |      Providers are the entities that publish listings to the Marketplace.\n",
      " |\n",
      " |  credentials\n",
      " |      A credential represents an authentication and authorization mechanism for accessing services on your cloud tenant.\n",
      " |\n",
      " |  credentials_manager\n",
      " |      Credentials manager interacts with with Identity Providers to to perform token exchanges using stored credentials and refresh tokens.\n",
      " |\n",
      " |  current_user\n",
      " |      This API allows retrieving information about currently authenticated user or service principal.\n",
      " |\n",
      " |  dashboard_widgets\n",
      " |      This is an evolving API that facilitates the addition and removal of widgets from existing dashboards within the Databricks Workspace.\n",
      " |\n",
      " |  dashboards\n",
      " |      In general, there is little need to modify dashboards using the API.\n",
      " |\n",
      " |  data_quality\n",
      " |      Manage the data quality of Unity Catalog objects (currently support `schema` and `table`).\n",
      " |\n",
      " |  data_sources\n",
      " |      This API is provided to assist you in making new query objects.\n",
      " |\n",
      " |  database\n",
      " |      Database Instances provide access to a database via REST API or direct SQL.\n",
      " |\n",
      " |  dbfs\n",
      " |      DBFS API makes it simple to interact with various data sources without having to include a users credentials every time to read a file.\n",
      " |\n",
      " |  dbsql_permissions\n",
      " |      The SQL Permissions API is similar to the endpoints of the :method:permissions/set.\n",
      " |\n",
      " |  dbutils\n",
      " |\n",
      " |  entity_tag_assignments\n",
      " |      Tags are attributes that include keys and optional values that you can use to organize and categorize entities in Unity Catalog.\n",
      " |\n",
      " |  experiments\n",
      " |      Experiments are the primary unit of organization in MLflow; all MLflow runs belong to an experiment.\n",
      " |\n",
      " |  external_lineage\n",
      " |      External Lineage APIs enable defining and managing lineage relationships between Databricks objects and external systems.\n",
      " |\n",
      " |  external_locations\n",
      " |      An external location is an object that combines a cloud storage path with a storage credential that authorizes access to the cloud storage path.\n",
      " |\n",
      " |  external_metadata\n",
      " |      External Metadata objects enable customers to register and manage metadata about external systems within Unity Catalog.\n",
      " |\n",
      " |  feature_engineering\n",
      " |      [description].\n",
      " |\n",
      " |  feature_store\n",
      " |      A feature store is a centralized repository that enables data scientists to find and share features.\n",
      " |\n",
      " |  files\n",
      " |      The Files API is a standard HTTP API that allows you to read, write, list, and delete files and directories by referring to their URI.\n",
      " |\n",
      " |  forecasting\n",
      " |      The Forecasting API allows you to create and get serverless forecasting experiments.\n",
      " |\n",
      " |  functions\n",
      " |      Functions implement User-Defined Functions (UDFs) in Unity Catalog.\n",
      " |\n",
      " |  genie\n",
      " |      Genie provides a no-code experience for business users, powered by AI/BI.\n",
      " |\n",
      " |  git_credentials\n",
      " |      Registers personal access token for Databricks to do operations on behalf of the user.\n",
      " |\n",
      " |  global_init_scripts\n",
      " |      The Global Init Scripts API enables Workspace administrators to configure global initialization scripts for their workspace.\n",
      " |\n",
      " |  grants\n",
      " |      In Unity Catalog, data is secure by default.\n",
      " |\n",
      " |  groups\n",
      " |      Groups simplify identity management, making it easier to assign access to Databricks workspace, data, and other securable objects.\n",
      " |\n",
      " |  groups_v2\n",
      " |      Groups simplify identity management, making it easier to assign access to Databricks workspace, data, and other securable objects.\n",
      " |\n",
      " |  instance_pools\n",
      " |      Instance Pools API are used to create, edit, delete and list instance pools by using ready-to-use cloud instances which reduces a cluster start and auto-scaling times.\n",
      " |\n",
      " |  instance_profiles\n",
      " |      The Instance Profiles API allows admins to add, list, and remove instance profiles that users can launch clusters with.\n",
      " |\n",
      " |  ip_access_lists\n",
      " |      IP Access List enables admins to configure IP access lists.\n",
      " |\n",
      " |  jobs\n",
      " |      The Jobs API allows you to create, edit, and delete jobs.\n",
      " |\n",
      " |  lakeview\n",
      " |      These APIs provide specific management operations for Lakeview dashboards.\n",
      " |\n",
      " |  lakeview_embedded\n",
      " |      Token-based Lakeview APIs for embedding dashboards in external applications.\n",
      " |\n",
      " |  libraries\n",
      " |      The Libraries API allows you to install and uninstall libraries and get the status of libraries on a cluster.\n",
      " |\n",
      " |  materialized_features\n",
      " |      Materialized Features are columns in tables and views that can be directly used as features to train and serve ML models.\n",
      " |\n",
      " |  metastores\n",
      " |      A metastore is the top-level container of objects in Unity Catalog.\n",
      " |\n",
      " |  model_registry\n",
      " |      Note: This API reference documents APIs for the Workspace Model Registry.\n",
      " |\n",
      " |  model_versions\n",
      " |      Databricks provides a hosted version of MLflow Model Registry in Unity Catalog.\n",
      " |\n",
      " |  notification_destinations\n",
      " |      The notification destinations API lets you programmatically manage a workspace's notification destinations.\n",
      " |\n",
      " |  online_tables\n",
      " |      Online tables provide lower latency and higher QPS access to data from Delta tables.\n",
      " |\n",
      " |  permission_migration\n",
      " |      APIs for migrating acl permissions, used only by the ucx tool: https://github.com/databrickslabs/ucx.\n",
      " |\n",
      " |  permissions\n",
      " |      Permissions API are used to create read, write, edit, update and manage access for various users on different objects and endpoints.\n",
      " |\n",
      " |  pipelines\n",
      " |      The Lakeflow Spark Declarative Pipelines API allows you to create, edit, delete, start, and view details about pipelines.\n",
      " |\n",
      " |  policies\n",
      " |      Attribute-Based Access Control (ABAC) provides high leverage governance for enforcing compliance policies in Unity Catalog.\n",
      " |\n",
      " |  policy_compliance_for_clusters\n",
      " |      The policy compliance APIs allow you to view and manage the policy compliance status of clusters in your workspace.\n",
      " |\n",
      " |  policy_compliance_for_jobs\n",
      " |      The compliance APIs allow you to view and manage the policy compliance status of jobs in your workspace.\n",
      " |\n",
      " |  policy_families\n",
      " |      View available policy families.\n",
      " |\n",
      " |  postgres\n",
      " |      Use the Postgres API to create and manage Lakebase Autoscaling Postgres infrastructure, including projects, branches, compute endpoints, and roles.\n",
      " |\n",
      " |  provider_exchange_filters\n",
      " |      Marketplace exchanges filters curate which groups can access an exchange.\n",
      " |\n",
      " |  provider_exchanges\n",
      " |      Marketplace exchanges allow providers to share their listings with a curated set of customers.\n",
      " |\n",
      " |  provider_files\n",
      " |      Marketplace offers a set of file APIs for various purposes such as preview notebooks and provider icons.\n",
      " |\n",
      " |  provider_listings\n",
      " |      Listings are the core entities in the Marketplace.\n",
      " |\n",
      " |  provider_personalization_requests\n",
      " |      Personalization requests are an alternate to instantly available listings.\n",
      " |\n",
      " |  provider_provider_analytics_dashboards\n",
      " |      Manage templated analytics solution for providers.\n",
      " |\n",
      " |  provider_providers\n",
      " |      Providers are entities that manage assets in Marketplace.\n",
      " |\n",
      " |  providers\n",
      " |      A data provider is an object representing the organization in the real world who shares the data.\n",
      " |\n",
      " |  quality_monitor_v2\n",
      " |      [DEPRECATED] This API is deprecated.\n",
      " |\n",
      " |  quality_monitors\n",
      " |      [DEPRECATED] This API is deprecated.\n",
      " |\n",
      " |  queries\n",
      " |      The queries API can be used to perform CRUD operations on queries.\n",
      " |\n",
      " |  queries_legacy\n",
      " |      These endpoints are used for CRUD operations on query definitions.\n",
      " |\n",
      " |  query_history\n",
      " |      A service responsible for storing and retrieving the list of queries run against SQL endpoints and serverless compute.\n",
      " |\n",
      " |  query_visualizations\n",
      " |      This is an evolving API that facilitates the addition and removal of visualizations from existing queries in the Databricks Workspace.\n",
      " |\n",
      " |  query_visualizations_legacy\n",
      " |      This is an evolving API that facilitates the addition and removal of vizualisations from existing queries within the Databricks Workspace.\n",
      " |\n",
      " |  recipient_activation\n",
      " |      The Recipient Activation API is only applicable in the open sharing model where the recipient object has the authentication type of `TOKEN`.\n",
      " |\n",
      " |  recipient_federation_policies\n",
      " |      The Recipient Federation Policies APIs are only applicable in the open sharing model where the recipient object has the authentication type of `OIDC_RECIPIENT`, enabling data sharing from Databricks to non-Databricks recipients.\n",
      " |\n",
      " |  recipients\n",
      " |      A recipient is an object you create using :method:recipients/create to represent an organization which you want to allow access shares.\n",
      " |\n",
      " |  redash_config\n",
      " |      Redash V2 service for workspace configurations (internal).\n",
      " |\n",
      " |  registered_models\n",
      " |      Databricks provides a hosted version of MLflow Model Registry in Unity Catalog.\n",
      " |\n",
      " |  repos\n",
      " |      The Repos API allows users to manage their git repos.\n",
      " |\n",
      " |  resource_quotas\n",
      " |      Unity Catalog enforces resource quotas on all securable objects, which limits the number of resources that can be created.\n",
      " |\n",
      " |  rfa\n",
      " |      Request for Access enables users to request access for Unity Catalog securables.\n",
      " |\n",
      " |  schemas\n",
      " |      A schema (also called a database) is the second layer of Unity Catalog’s three-level namespace.\n",
      " |\n",
      " |  secrets\n",
      " |      The Secrets API allows you to manage secrets, secret scopes, and access permissions.\n",
      " |\n",
      " |  service_principal_secrets_proxy\n",
      " |      These APIs enable administrators to manage service principal secrets at the workspace level.\n",
      " |\n",
      " |  service_principals\n",
      " |      Identities for use with jobs, automated tools, and systems such as scripts, apps, and CI/CD platforms.\n",
      " |\n",
      " |  service_principals_v2\n",
      " |      Identities for use with jobs, automated tools, and systems such as scripts, apps, and CI/CD platforms.\n",
      " |\n",
      " |  serving_endpoints\n",
      " |      The Serving Endpoints API allows you to create, update, and delete model serving endpoints.\n",
      " |\n",
      " |  serving_endpoints_data_plane\n",
      " |      Serving endpoints DataPlane provides a set of operations to interact with data plane endpoints for Serving endpoints service.\n",
      " |\n",
      " |  settings\n",
      " |      Workspace Settings API allows users to manage settings at the workspace level.\n",
      " |\n",
      " |  shares\n",
      " |      A share is a container instantiated with :method:shares/create.\n",
      " |\n",
      " |  statement_execution\n",
      " |      The Databricks SQL Statement Execution API can be used to execute SQL statements on a SQL warehouse and fetch the result.\n",
      " |\n",
      " |  storage_credentials\n",
      " |      A storage credential represents an authentication and authorization mechanism for accessing data stored on your cloud tenant.\n",
      " |\n",
      " |  system_schemas\n",
      " |      A system schema is a schema that lives within the system catalog.\n",
      " |\n",
      " |  table_constraints\n",
      " |      Primary key and foreign key constraints encode relationships between fields in tables.\n",
      " |\n",
      " |  tables\n",
      " |      A table resides in the third layer of Unity Catalog’s three-level namespace.\n",
      " |\n",
      " |  tag_policies\n",
      " |      The Tag Policy API allows you to manage policies for governed tags in Databricks.\n",
      " |\n",
      " |  temporary_path_credentials\n",
      " |      Temporary Path Credentials refer to short-lived, downscoped credentials used to access external cloud storage locations registered in Databricks.\n",
      " |\n",
      " |  temporary_table_credentials\n",
      " |      Temporary Table Credentials refer to short-lived, downscoped credentials used to access cloud storage locations where table data is stored in Databricks.\n",
      " |\n",
      " |  token_management\n",
      " |      Enables administrators to get all tokens and delete tokens for other users.\n",
      " |\n",
      " |  tokens\n",
      " |      The Token API allows you to create, list, and revoke tokens that can be used to authenticate and access Databricks REST APIs.\n",
      " |\n",
      " |  users\n",
      " |      User identities recognized by Databricks and represented by email addresses.\n",
      " |\n",
      " |  users_v2\n",
      " |      User identities recognized by Databricks and represented by email addresses.\n",
      " |\n",
      " |  vector_search_endpoints\n",
      " |      **Endpoint**: Represents the compute resources to host vector search indexes.\n",
      " |\n",
      " |  vector_search_indexes\n",
      " |      **Index**: An efficient representation of your embedding vectors that supports real-time and efficient approximate nearest neighbor (ANN) search queries.\n",
      " |\n",
      " |  volumes\n",
      " |      Volumes are a Unity Catalog (UC) capability for accessing, storing, governing, organizing and processing files.\n",
      " |\n",
      " |  warehouses\n",
      " |      A SQL warehouse is a compute resource that lets you run SQL commands on data objects within Databricks SQL.\n",
      " |\n",
      " |  workspace\n",
      " |      The Workspace API allows you to list, import, export, and delete notebooks and folders.\n",
      " |\n",
      " |  workspace_bindings\n",
      " |      A securable in Databricks can be configured as __OPEN__ or __ISOLATED__.\n",
      " |\n",
      " |  workspace_conf\n",
      " |      This API allows updating known workspace settings for advanced users.\n",
      " |\n",
      " |  workspace_entity_tag_assignments\n",
      " |      Manage tag assignments on workspace-scoped objects.\n",
      " |\n",
      " |  workspace_iam_v2\n",
      " |      These APIs are used to manage identities and the workspace access of these identities in <Databricks>.\n",
      " |\n",
      " |  workspace_settings_v2\n",
      " |      APIs to manage workspace level settings.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(WorkspaceClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Authentication using Databricks Service Principal\n",
    "# Service principals are recommended for automation, CI/CD pipelines, and scripts\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AUTHENTICATE USING DATABRICKS SERVICE PRINCIPAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Method 1: Using environment variables\n",
    "print(\"\\nMethod 1: Using Environment Variables\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Set these environment variables with your service principal credentials:\n",
    "# os.environ['DATABRICKS_HOST'] = 'https://your-workspace.databricks.com'\n",
    "# os.environ['DATABRICKS_CLIENT_ID'] = 'your-client-id'\n",
    "# os.environ['DATABRICKS_CLIENT_SECRET'] = 'your-client-secret'\n",
    "\n",
    "# Then initialize the client:\n",
    "# client_sp = WorkspaceClient()\n",
    "# print(\"Client initialized successfully with service principal\")\n",
    "\n",
    "# Method 2: Pass credentials directly in the code\n",
    "print(\"\\nMethod 2: Passing Credentials Directly\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# client_sp = WorkspaceClient(\n",
    "#     host='https://your-workspace.databricks.com',\n",
    "#     client_id='your-client-id',\n",
    "#     client_secret='your-client-secret'\n",
    "# )\n",
    "# print(\"Client initialized successfully with service principal\")\n",
    "\n",
    "# Method 3: Using Azure Service Principal (if using Azure Databricks)\n",
    "print(\"\\nMethod 3: Using Azure Service Principal\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# For Azure Databricks with Service Principal:\n",
    "# client_sp = WorkspaceClient(\n",
    "#     host='https://your-workspace.databricks.com',\n",
    "#     client_id='your-app-id',\n",
    "#     client_secret='your-client-secret',\n",
    "#     tenant_id='your-tenant-id'  # Azure tenant ID\n",
    "# )\n",
    "# print(\"Client initialized successfully with Azure service principal\")\n",
    "\n",
    "# Method 4: Using .databrickscfg with named configuration\n",
    "print(\"\\nMethod 4: Using .databrickscfg File\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create/edit ~/.databrickscfg file with:\n",
    "# [DEFAULT]\n",
    "# host = https://your-workspace.databricks.com\n",
    "# client_id = your-client-id\n",
    "# client_secret = your-client-secret\n",
    "\n",
    "# Then use:\n",
    "# client_sp = WorkspaceClient()\n",
    "# print(\"Client initialized from .databrickscfg\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"To get service principal credentials:\")\n",
    "print(\"1. Go to Databricks Admin Console > Service Principals\")\n",
    "print(\"2. Create a new Service Principal\")\n",
    "print(\"3. Generate OAuth Secret\")\n",
    "print(\"4. Copy Client ID and Client Secret\")\n",
    "print(\"5. Use them for authentication as shown above\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize the Databricks Workspace Client\n",
    "# Make sure you have DATABRICKS_HOST and DATABRICKS_TOKEN environment variables set\n",
    "# or provide credentials explicitly\n",
    "client = WorkspaceClient()\n",
    "\n",
    "# Create a personal access token with full \"all-purpose\" scope\n",
    "token_response = client.token_management.create_token(\n",
    "    comment=\"Personal token with full control\",\n",
    "    lifetime_seconds=7776000  # 90 days in seconds\n",
    ")\n",
    "\n",
    "# Extract and display the token details\n",
    "print(f\"Token ID: {token_response.token_id}\")\n",
    "print(f\"Token (save this securely): {token_response.token_value}\")\n",
    "print(f\"Creation Time: {token_response.creation_time}\")\n",
    "print(f\"Expiration Time: {token_response.expiration_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointInfo(auto_stop_mins=10, channel=Channel(dbsql_version=None, name=<ChannelName.CHANNEL_NAME_CURRENT: 'CHANNEL_NAME_CURRENT'>), cluster_size='2X-Small', creator_name='niladridasgit@gmail.com', enable_photon=True, enable_serverless_compute=True, health=EndpointHealth(details=None, failure_reason=None, message=None, status=<Status.HEALTHY: 'HEALTHY'>, summary=None), id='fe01e6b774500185', instance_profile_arn=None, jdbc_url='jdbc:spark://dbc-1d41a383-c4d0.cloud.databricks.com:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/fe01e6b774500185;', max_num_clusters=1, min_num_clusters=1, name='Serverless Starter Warehouse', num_active_sessions=None, num_clusters=1, odbc_params=OdbcParams(hostname='dbc-1d41a383-c4d0.cloud.databricks.com', path='/sql/1.0/warehouses/fe01e6b774500185', port=443, protocol='https'), spot_instance_policy=<SpotInstancePolicy.COST_OPTIMIZED: 'COST_OPTIMIZED'>, state=<State.RUNNING: 'RUNNING'>, tags=EndpointTags(custom_tags=[]), warehouse_type=<EndpointInfoWarehouseType.PRO: 'PRO'>)\n"
     ]
    }
   ],
   "source": [
    "for cluster in client.clusters.list():\n",
    "    print(cluster)\n",
    "\n",
    "for warehouse in client.warehouses.list():\n",
    "    print(warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseJob(created_time=1753704772646, creator_user_name='niladridasgit@gmail.com', effective_budget_policy_id=None, effective_usage_policy_id=None, has_more=None, job_id=302706857201310, settings=JobSettings(budget_policy_id=None, continuous=None, deployment=None, description=None, edit_mode=None, email_notifications=JobEmailNotifications(no_alert_for_skipped_runs=False, on_duration_warning_threshold_exceeded=None, on_failure=None, on_start=None, on_streaming_backlog_exceeded=None, on_success=None), environments=[], format=<Format.MULTI_TASK: 'MULTI_TASK'>, git_source=None, health=None, job_clusters=[], max_concurrent_runs=1, name='job', notification_settings=None, parameters=[], performance_target=<PerformanceTarget.STANDARD: 'STANDARD'>, queue=QueueSettings(enabled=True), run_as=None, schedule=None, tags=None, tasks=[], timeout_seconds=0, trigger=None, usage_policy_id=None, webhook_notifications=None), trigger_state=None)\n",
      "PipelineStateInfo(cluster_id=None, creator_user_name='niladridasgit@gmail.com', health=None, latest_updates=[UpdateStateInfo(creation_time='2026-01-27T14:29:56.469Z', state=<UpdateStateInfoState.FAILED: 'FAILED'>, update_id='2d15e78b-96d6-49d0-8886-ac7a5ef5847b'), UpdateStateInfo(creation_time='2026-01-27T14:28:38.589Z', state=<UpdateStateInfoState.FAILED: 'FAILED'>, update_id='5b2beaf4-3f59-48bc-a576-74d81bca979d'), UpdateStateInfo(creation_time='2026-01-27T14:27:33.868Z', state=<UpdateStateInfoState.FAILED: 'FAILED'>, update_id='25854690-d54e-47bd-9edc-2975e7e6ca91'), UpdateStateInfo(creation_time='2026-01-27T14:26:19.558Z', state=<UpdateStateInfoState.FAILED: 'FAILED'>, update_id='c2852b53-0120-4c92-ab63-d8adbc2416f2'), UpdateStateInfo(creation_time='2026-01-27T14:25:02.298Z', state=<UpdateStateInfoState.FAILED: 'FAILED'>, update_id='4db5f95d-9504-480e-85a7-e92d741530c6')], name='alpha', pipeline_id='34eae018-a522-461d-a813-900b7c1cfa68', run_as_user_name='niladridasgit@gmail.com', state=<PipelineState.IDLE: 'IDLE'>)\n",
      "PipelineStateInfo(cluster_id=None, creator_user_name='niladridasgit@gmail.com', health=None, latest_updates=[], name='first_pipeline', pipeline_id='6d65c297-0aae-406e-a9a1-1eee35af3848', run_as_user_name='niladridasgit@gmail.com', state=<PipelineState.IDLE: 'IDLE'>)\n"
     ]
    }
   ],
   "source": [
    "for job in client.jobs.list():\n",
    "    print(job)\n",
    "\n",
    "for pipeline in client.pipelines.list_pipelines():\n",
    "    print(pipeline)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6352130658015451,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "All About - Databricks Asset Bundles",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
